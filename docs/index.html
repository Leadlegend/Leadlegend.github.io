<!DOCTYPE html>
<html><head>
	<meta name="generator" content="Hugo 0.92.1" />
  <title>Kaixin Zhang</title>
  <link
    href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,400;0,600;0,700;1,400;1,600;1,700&display=swap"
    rel="stylesheet"
  />

  
  <link rel="stylesheet" href="main.css" />

  <link
    rel="stylesheet"
    href="https://use.fontawesome.com/releases/v5.1.0/css/all.css"
    integrity="sha384-lKuwvrZot6UHsBSfcMvOkWwlCMgc0TaWr+30HWe3a4ltaBwTZhyTEggF5tJv8tbt"
    crossorigin="anonymous"
  />
</head>
<body>
  <main>
    
      <div class="box">
        <div class="wrap">
          <header>
  <div class="page-title">
    <h1>Kaixin Zhang</h1>
    <div class="address"></div>
  </div>
  <div>
    
    <a href="mailto:kaixinzhang@pku.edu.cn">kaixinzhang@pku.edu.cn</a>
    
    
    <a href="tel:&#43;86%2019801203004">(&#43;86) 198-0120-3004</a>
    
    
    <a href="https://kaixinzhang.org">kaixinzhang.org</a>
    
    
    <a href="https://github.com/Leadlegend"><i class="fab fa-github"></i> Leadlegend</a>
    
  </div>
</header>
           <section>
  <h2>Education</h2>
  
  

  
    <div>
      
        <div class="heading-line">
          <h3>AI Turing Class, EECS College, Peking University - Beijing, China</h3>
          <h3>
            Sep 2018 -
            Present
          </h3>
        </div>
      
      
        <p>
          <em>Bachelor of Engneering in Artificial Intelligence</em>
        </p>
      
      

    </div>
  
    <div>
      
        <div class="heading-line">
          <h3>Overall GPA - 3.67/4</h3>
          <h3>
            Major GPA -
            3.72/4
          </h3>
        </div>
      
      
        <p>
          <em>Major Rank - 23 / 81 (30%)</em>
        </p>
      
      
  <ul>
    
    
      
        <li>
          <strong>Honor:</strong>
          
          
            
  <ul>
    
    
      
        <li>
          <strong>Newbee Scholarship of Peking University</strong> - <em>Dec 2018</em>
          
          
        </li>
      
    
      
        <li>
          <strong>School-level Scholarship of Peking University</strong> - <em>Oct 2019</em>
          
          
        </li>
      
    
  </ul>


          
        </li>
      
    
      
    
  </ul>


    </div>
  
</section>
  <section>
  <h2>Research Interests</h2>
  
  
  <ul>
    
    
      
        <li>
          Knowledge-based Natural Language Processing (NER, QA, entity linking, knowledge graph)
          
          
        </li>
      
    
      
        <li>
          Limited-data learning (meta-learning, transfer learning, weak-supervised learning)
          
          
        </li>
      
    
      
        <li>
          Language Model Pretraining
          
          
        </li>
      
    
      
        <li>
          Natural Language Processing Applications to Biology and Medicine
          
          
        </li>
      
    
  </ul>


  
</section>
  <section>
  <h2>Research Experience</h2>
  
  

  
    <div>
      
        <div class="heading-line">
          <h3>Interpretable Neural Network for Drug Response Prediction - Remote Online Research</h3>
          <h3>
            Apr 2022 -
            Present
          </h3>
        </div>
      
      
        <p>
          <em>Research Assistant | Supervisor: Assistant Prof. Sheng Wang, Paul G. Allen School of CSE, University of Washington</em>
        </p>
      
      
  <ul>
    
    
      
        <li>
          Carefully investigated the interpretability of neural networks, a critical problem in BioNLP, and especially paid attention to the way of presentation of interpretability, such as <a href="http://d-cell.ucsd.edu">visible neural networks</a>.
          
          
        </li>
      
    
      
        <li>
          Reproduced <a href="http://drugcell.ucsd.edu/landing/">DrugCell</a>, a canonical interpretable model for drug response prediction on cancer. Conducted further study for this task and optimized DrugCell&rsquo;s prediction performance. (<a href="https://github.com/Leadlegend/DrugCell">Repository Link</a>)
          
          
        </li>
      
    
      
        <li>
          Explored a new method to yield insights of how neural network encodes its neurons: Readable Neural Networks. Extracted contextual text embeddings of <a href="http://geneontology.org">Gene Ontology</a> terms from <a href="https://pubmed.ncbi.nlm.nih.gov">PubMed</a> literatures through distant supervision, and introduced the text feature into DrugCell to enhance model interpretability without losing its predicting accuracy.
          
          
        </li>
      
    
      
        <li>
          For the next step, we plan to train a decoding module for DrugCell to generate auxiliary texts of its prediction, which can help researcher get a better insight of working mechanism of neural network through reading.
          
          
        </li>
      
    
      
        <li>
          Also, inspired by <a href="https://arxiv.org/abs/2007.06559">graph2nn</a>, we plan to explore the relationship between graph structure of neural networks and its performance on drug response prediction task. We will try to design a dfferent algorithm to convert Gene Ontology into neural networks, which can implement better performance.
          
          
        </li>
      
    
  </ul>


    </div>
  
    <div>
      
        <div class="heading-line">
          <h3>Development of Commonsense-based Question Generation Models - Beijing, China</h3>
          <h3>
            Apr 2020 -
            Nov 2020
          </h3>
        </div>
      
      
        <p>
          <em>Research Assistant | Supervisor: Prof. Yunfang Wu, Institute of Computational Linguistics of PKU</em>
        </p>
      
      
  <ul>
    
    
      
        <li>
          Developed a seq-to-seq Question Generation model and designed static graph attention mechansim that extracts extern knowledge from <em>Knowledge Graph</em> and encodes context-irrelevant knowledge embedding as linguistic feature.
          
          
        </li>
      
    
      
        <li>
          Participated in a multitask learning QG project, provided assistance about baseline implementation and dataset transfer (from <em>SQuAD</em> to <a href="https://arxiv.org/abs/1704.04683"><em>RACE</em></a>) in ablation experiments.
          
          
        </li>
      
    
      
        <li>
          Reviewed development of pre-trained NLG methods (<em>BERTsum</em>, <em>BART</em>, <em>ProphetNet</em> and <em>PEGASus</em>), especially focusing on Text Summarization. Discussed feasible ways to introduce pretraining into question generation task.
          
          
        </li>
      
    
  </ul>


    </div>
  
    <div>
      
        <div class="heading-line">
          <h3>Review of Semi-supervised Text Classification Methods - Beijing, China</h3>
          <h3>
            Jan 2020 -
            Mar 2020
          </h3>
        </div>
      
      
        <p>
          <em>Member | Supervisor: Assistant Prof. Rui Yan, Wangxuan Institute of Computer Technology of PKU</em>
        </p>
      
      
  <ul>
    
    
      
        <li>
          Investigated semi-supervised and self-supervised text classification methods between 2018 and 2020, discussed the mechanism of self-supervised networks and further modification direction.
          
          
        </li>
      
    
      
        <li>
          Verified the validation of a noisy-label adversarial text classification method by model reproduction in PyTorch.
          
          
        </li>
      
    
  </ul>


    </div>
  
</section>
  <section>
  <h2>Professional Experience</h2>
  
  

  
    <div>
      
        <div class="heading-line">
          <h3>Tencent Co., Ltd - Beijing, China</h3>
          <h3>
            Apr 2021 -
            Dec 2021
          </h3>
        </div>
      
      
        <p>
          <em>Research Intern | Department: TEG AI Lab &amp; AI Platform Department</em>
        </p>
      
      
  <ul>
    
    
      
        <li>
          Participated in maintenance of an universal-domain knowledge graph <a href="https://blog.titanwolf.in/a?ID=01700-97563bfe-6be7-43fc-88d0-eada389cf07c">Topbase</a>, optimized sampling strategy in construction of a paraellel dataset for entity linking to mitigate influence of severe label bias.
          
          
        </li>
      
    
      
        <li>
          Implemented multi-GPU(ddp) gradient gathering mechanism for a bi-encoder based Entity Linking model, which supplied bigger batch size and more negative samples, thus improving model performance.
          
          
        </li>
      
    
      
        <li>
          Transferred a contrastive learning paradigm <a href="https://arxiv.org/abs/2102.12903">Self-Tuning</a> to domain-specific semi-supervised NER models, introduced Training Signal Annealing and self-distillation to utilize unlabeled data more efficiently.
          
          
        </li>
      
    
      
        <li>
          Implemented an keyword-to-text model generating advertisement based on chinese <em>GPT-2</em> and <a href="https://github.com/dbiir/UER-py">UER-py</a>, optimized keyword coverage and generation diversity via in-domain pretraining and <a href="https://aclanthology.org/2021.acl-long.9.pdf">Mention Flags</a>, which can encode keyword mention status into multi-head attentions of transformer encoder.
          
          
        </li>
      
    
      
        <li>
          Above-mentioned advertisement generation project received “Tencent Monthly Innovation Award” and was applied to Tencent advertisement business of online reading.
          
          
        </li>
      
    
  </ul>


    </div>
  
</section>
  <section>
  <h2>Project Development</h2>
  
  

  
    <div>
      
        <div class="heading-line">
          <h3>Commonsense-based Question Generation Model</h3>
          <h3>
            Jul 2020 -
            Oct 2020
          </h3>
        </div>
      
      
        <p>
          <em>Leader | Supervisor: Prof. Yunfang Wu, Institute of Computational Linguistics of PKU</em>
        </p>
      
      
  <ul>
    
    
      
        <li>
          Implemented as standard seq-to-seq architecture in Bi-LSTM and adopted fundamental mechanisms in QG including Copy Pointer and Gated Self-Attention.(<a href="https://github.com/Leadlegend/Commonsense-based-Question-Generation">Repository Link</a>)
          
          
        </li>
      
    
      
        <li>
          Designed a static graph attention to generate knowledge embedding from Knowledge Graph (embedded by <em>TransE</em>), which can be viewed as additional linguistic feature and concatenated onto word embedding.
          
          
        </li>
      
    
      
        <li>
          Our method reached 17.47 for BLEU-4 on <em>SQuAD</em>, which is distinguished among non-pretrained methods, and can capture more implicit relations among knowledge entities according to human evaluation.
          
          
        </li>
      
    
  </ul>


    </div>
  
</section>
  <section>
  <h2>Skills</h2>
  
  
  <ul>
    
    
      
        <li>
          Pytorch, Pytorch-Lightning, Huggingface-Transformers, Python, C++, Tensorflow, Assembly
          
          
        </li>
      
    
  </ul>


  
</section>
 
        </div>
      </div>
    
  </main>

  </body>
</html>
