<!DOCTYPE html>
<html><head>
	<meta name="generator" content="Hugo 0.83.1" />
  <title>Kaixin Zhang</title>
  <link
    href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,400;0,600;0,700;1,400;1,600;1,700&display=swap"
    rel="stylesheet"
  />

  
  <link rel="stylesheet" href="main.css" />

  <link
    rel="stylesheet"
    href="https://use.fontawesome.com/releases/v5.1.0/css/all.css"
    integrity="sha384-lKuwvrZot6UHsBSfcMvOkWwlCMgc0TaWr+30HWe3a4ltaBwTZhyTEggF5tJv8tbt"
    crossorigin="anonymous"
  />
</head>
<body>
  <main>
    
      <div class="box">
        <div class="wrap">
          <header>
  <div class="page-title">
    <h1>Kaixin Zhang</h1>
    <div class="address"></div>
  </div>
  <div>
    
    <a href="mailto:kaixinzhang@pku.edu.cn">kaixinzhang@pku.edu.cn</a>
    
    
    <a href="tel:&#43;86%2019801203004">(&#43;86) 198-0120-3004</a>
    
    
    <a href="https://kaixinzhang.org">kaixinzhang.org</a>
    
    
    <a href="https://github.com/Leadlegend"><i class="fab fa-github"></i> Leadlegend</a>
    
  </div>
</header>
           <section>
  <h2>Education</h2>
  
  
  
    <div>
      
        <div class="heading-line">
          <h3>Peking University - Beijing, China</h3>
          <h3>
            Sep 2018 -
            Jul 2023
          </h3>
        </div>
      
      
        <p>
          <em>B.S. in Artificial Intelligence</em>
        </p>
      
      
    </div>
  
    <div>
      
        <div class="heading-line">
          <h3>Overall GPA - 3.67/4</h3>
          <h3>
            Major GPA -
            3.72/4
          </h3>
        </div>
      
      
        <p>
          <em>Newbee Scholarship of Peking University</em>
        </p>
      
        <p>
          <em>School-level Scholarship of Peking University</em>
        </p>
      
      
    </div>
  
</section>
  <section>
  <h2>Research Experience</h2>
  
  
  
    <div>
      
        <div class="heading-line">
          <h3>Development of Commonsense-based Question Generation Models - Beijing, China</h3>
          <h3>
            Apr 2020 -
            Nov 2020
          </h3>
        </div>
      
      
        <p>
          <em>Research Assistant | Supervisor: Prof. Yunfang Wu, Institute of Computational Linguistics of PKU </em>
        </p>
      
      
        <ul>
          
            <li>Developed a seq-to-seq Question Generation model and designed static graph attention mechansim that extracts extern knowledge from Knowledge Graph and encodes context-irrelevant knowledge embedding as linguistic feature.</li>
          
            <li>Participated in a Multitask Learning QG project, provided assistance about baseline implementation and dataset transfer (from SQuAD to RACE) in ablation experiments, which was finally accepted by ACL2020.</li>
          
            <li>Reviewed development of pre-trained NLG methods (BERTsum, BART, ProphetNet and PEGASus), especially focusing on Text Summarization. Discussed feasible ways to introduce pretraining into question generation task.</li>
          
        </ul>
      
    </div>
  
    <div>
      
        <div class="heading-line">
          <h3>Review of Semi-supervised and Self-supervised Text Classification Methods - Beijing, China</h3>
          <h3>
            Jan 2020 -
            Mar 2020
          </h3>
        </div>
      
      
        <p>
          <em>Member | Supervisor: Assistant Prof. Rui Yan, Wangxuan Institute of Computer Technology of PKU</em>
        </p>
      
      
        <ul>
          
            <li>Investigated semi-supervised and self-supervised text classification methods before 2020, discussed the mechanism of self-supervised networks and further modification direction.</li>
          
            <li>Verified the validation of a noisy-label adversarial text classification method by model reproduction in PyTorch.</li>
          
        </ul>
      
    </div>
  
</section>
  <section>
  <h2>Professional Experience</h2>
  
  
  
    <div>
      
        <div class="heading-line">
          <h3>Tencent Co., Ltd - Beijing, China</h3>
          <h3>
            Apr 2021 -
            Dec 2021
          </h3>
        </div>
      
      
        <p>
          <em>Research Intern | Department: TEG AI Lab &amp; AI Platform Department</em>
        </p>
      
      
        <ul>
          
            <li>Participated in maintenance of an universal-domain knowledge graph Topbase, optimized sampling strategy in construction of a paraellel dataset for entity linking to mitigate influence of severe label bias.</li>
          
            <li>Implemented multi-GPU(ddp) gradient gathering mechanism for a bi-encoder based EntityLinking model, which supplied bigger batch size and more negative samples, thus improving model performance.</li>
          
            <li>Transferred a contrastive learning paradigm Self-Tuning to domain-specific semi-supervised NER models, introduced Training Signal Annealing and self-distillation to utilize unlabeled data more efficiently.</li>
          
            <li>Implemented an keyword-to-text model generating advertisement based on chinese GPT-2 and UER-py, optimized keyword coverage and generation diversity via in-domain pretraining and Mention Flags, which can encode keyword mention status into multi-head attentions of transformer encoder.</li>
          
            <li>Above-mentioned advertisement generation project received “Tencent Monthly Innovation Award” and was applied to Tencent advertisement business of online reading.</li>
          
        </ul>
      
    </div>
  
</section>
  <section>
  <h2>Project Development</h2>
  
  
  
    <div>
      
        <div class="heading-line">
          <h3>Commonsense-based Question Generation Model</h3>
          <h3>
            Jul 2020 -
            Oct 2020
          </h3>
        </div>
      
      
        <p>
          <em>Leader | Supervisor: Prof. Yunfang Wu, Institute of Computational Linguistics of PKU </em>
        </p>
      
      
        <ul>
          
            <li>Implemented standard seq-to-seq architecture in Bi-LSTM and adopted fundamental mechanisms in QG including Copy Pointer and Gated Self-Attention.</li>
          
            <li>Designed a static graph attention to generate knowledge embedding from Knowledge Graph (embedded by TransE), which can be viewed as additional linguistic feature and concatenated onto word embedding.</li>
          
            <li>Our method reached 17.47 for BLEU-4 on SQuAD, which is distinguished among non-pretrained methods, and can capture more implicit relations among knowledge entities according to human evaluation.</li>
          
        </ul>
      
    </div>
  
</section>
  <section>
  <h2>Skills</h2>
  
  
    <ul>
      
        <li>Pytorch, Pytorch-Lightning, Huggingface-Transformers, Python, C++, Tensorflow, Assembly</li>
      
    </ul>
  
  
</section>
 
        </div>
      </div>
    
  </main>

  </body>
</html>
