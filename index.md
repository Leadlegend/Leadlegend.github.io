# <p align="center">Kaixin Zhang</p>

<p align="center">**Addr**: Peking University, No. 5 Yiheyuan Street, Haidian District,
Beijing, China, 100080</p>

<p align="center">TEL: +(86) 19801203004 || Email: PKUKaixinZhang@gmail.com</p>

## **EDUCATION**

### **AI Turing Class, EECS College, Peking University**                             
<p align="right">*Beijing, China*</p>

***Bachelor of Engneering in Artificial Intelligence***                         
<p align="right">*Sept. 2018 – Present*</p>

- **Overall GPA**: 3.67/4.00                <p align="right">**Major GPA**: 3.72/4.00</p>

- **Honor**: 
    - Newbee Scholarship of Peking University <p align="right">*Dec. 2018*</p>
    - School-level Scholarship of Peking University <p align="right">*Oct. 2019*</p>

- **Major courses**

    - Core Courses:
        - Machine Learning 88/100,
        - Operating System 86/100,
        - Discrete Mathematics and Structures 94/100

    - Frontier Computing Courses:
        - Natural language and Data science 100/100,
        - The Brain and Cognitive Science 91/100,
        - Study and Practice on Topics of Frontier Computing 90/100

## **RESEARCH EXPERIENCE**

### **Development of Commonsense-based Question Generation Models**                 
<p align="right">*May. 2020-Nov. 2020, Beijing*</p>

*Research Assistant* || *Supervisor: Prof. Yunfang Wu, Institute of Computational Linguistics of PKU*

-   Designed a Seq-to-seq Question Generation model and developed static graph attention mechansim that extracts extern knowledge from Knowledge Graph to assist feature capture of the encoder

-   Transferred an existing QG model from SQuAD (a classic NLG corpus) to RACE (the latest NLG corpus)

-   Participated in another project of Commonsense-based Multitask Learning QG project, provided assistance about baseline implementation and data completion in ablation experiment, which was accepted by *ACL2020*

### **Study of Mainstream Methods for Continual Learning**                          
<p align="right">*Mar. 2020-May. 2020, Beijing*</p>

*Member* || *Supervisor: Prof. Zhihong Deng, Department of Machine Intelligence of PKU*

-   Participated in seminars with the research group, and delivered speech about future of replay-based Contiual Learning, especially about knowledge storage and transference

-   Finished a detailed paper review covering the origin and development of three classes methods of Lifelong Learning: regularization-based, dynamic-structure-based and replay-based neural network training

### **Modification of Semi and Self-supervised Text Classification Methods**        
<p align="right">*Jan. 2020-Mar. 2020, Beijing*</p>

*Member* || *Supervisor: Assistant Prof. Rui Yan,* *Wangxuan Institute of Computer Technology of PKU*

-   Investigated all the published semi-supervised and unsupervised Text Classification models from 2018 to 2020, and discussed the working mechanism of some self-supervised networks and possibility of optimization

-   Verified the validation of one of the noisy-label adversarial TC models through model reproduction using PyTorch

-   Designed an feasible method to integrate Paragraph-level Features to improve TC.

## **PROJECT DEVELOPMENT**

### ***Commonsense Question Generation Model using Knowledge Graph***               
<p align="right">*July. 2020-Oct. 2020, Beijing*</p>

*Leader* || *Supervisor: Prof. Yunfang Wu, Institute of Computational Linguistics of PKU*

-   Reached 17.63 for BLEU-4, on standard SQuAD test set, which is distinguished among the non-pretrained methods

-   Designed a static graph attention mechanism to integrate extern information from Knowledge Graph, which can be concatenated onto word embeddings as linguistic feature to assist decoder to generate more “rational” tokens

-   Adopted classic mechanisms of seq-to-seq QG including Maxout Pointer Copy Mechansim and Gated Self Attention

-   For the next step, to implement some other attention structures to extract further commonsense information and will pay more attention to pre-train models

### **Fracture Detection Model using Faster-RCNN** 
<p align="right">*Apr. 2020-June. 2020, Beijing*</p>

*Member* || *Supervisor: Prof. Liwei Wang, Center for Data Science of PKU*

-   Used Generalizaed-RCNN of Detectron2 to implemented Faster-RCNN for its scalability, training Backbone Network for feature extraction, Region Proposal Network for anchor detection and ROI pooling layer for length normalization

-   After investigation and comparision, we took ResNet+FPN as backbone network instead of purely ResNet, which paid comprehensive attention to features in all levels

-   Adopted transfer learning on ImageNet to get better performance, specifically, fixing parameters of downside layers of the backbone, and fine-tune the parameters of remains

## **SKILLS**

Pytorch, C, C++, Python, Assembly, Tensorflow

